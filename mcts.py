import config as cfg
import numpy as np
import time
import torch
"""
First attempt to not use a node class to perform tree search
class Node:
    def __init__(self, parent=None, state=None):
        self.parent = parent
        self.state = state

        # Expand this when tree policy reaches this node
        self.children = []
        self.visited = 0
        self.value = 0
"""


class MCTS:
    def __init__(self, game, ann, eps=0.1, c_ucb=1.2, search_time=2):
        self.game = game
        self.ann = ann
        self.eps = eps
        self.c_ucb = c_ucb
        self.search_time = search_time

        self.size = game.get_game_size()
        self.Qsa = {}  # stores Q values for edge s,a
        self.Nsa = {}  # stores #times edge s,a was visited
        self.Ns = {}  # stores #times state s was visited
        self.Ps = {}

    def getActionProb(self, state, search=True):
        """
        Function to get the target distribution of action probabilities
        for a given state. Target distribution is generated by playing
        a series of rollout games from the given state. This is added to
        the replay buffer in trainer.py, and subsequently used for training
        the ann after enough games are played.
        """
        if search:
            # Perform MCTS
            i = 0
            time_end = time.time() + self.search_time  # Search games for x seconds
            # Search for max 400 iterations and at least num_board_cells iterations
            while (i < 400 and time.time() < time_end) or i < 14:
                i += 1
                self.search(state)

            # Get visit counts for all edges going out from the root state
            valids = self.game.generate_legal_moves(state)
            counts = [self.Nsa[(state, a)] if (state, a)
                      in self.Nsa else 0 for a in range(len(valids))]
            sum_counts = np.sum(counts)
           
            # Return normalized visit counts as a probability distribution
            return np.array([c / sum_counts for c in counts]) 

        # Pick move based on model eval without any search
        nn_input = self.ann.convert_state_to_input(state, self.size)
        with torch.no_grad():
            preds = self.ann.forward(nn_input).detach().numpy().flatten()

        valids = self.game.generate_legal_moves(state)
        valid_preds = preds * valids  # Mask illegal moves
        sum_Ps = np.sum(valid_preds)
        if sum_Ps > 0:
            valid_preds /= sum_Ps  # renormalize
        else:
            # if all valid moves were masked make all valid moves equally probable
            print("All valid moves were masked, nn may be overfitting.")
            valid_preds += valids
            valid_preds /= np.sum(valid_preds)

        return valid_preds

    def search(self, state):
        """
        Performs one rollout iteration of the MCTS.
        The function is called recursively using the tree policy
        until a leaf node or a terminal state is found.

        If a non-terminal leaf node is reached, then we perform a 
        rollout game from that leaf node using the default policy
        (the ann predictions) until we reach a final state. The 
        result is then backpropagated up the search path
        """

        # Check if the state is a terminal state
        game_result = self.game.check_game_ended(state)
        if game_result:
            # Return +1 if player 1 won the game,
            # and return -1 if player 2 won the game
            return 1 if game_result == 1 else -1

        # Check if state is a leaf node
        # If it is, then perform rollout
        if state not in self.Ns:
            self.Ns[state] = 0
            return self.rollout(state)

        # Use tree policy to recursively search for a leaf node
        # First generate all legal moves from current state
        valids = self.game.generate_legal_moves(state)
        # Get the player turn from the state, as player 1 wants
        # maximize best_Qu, while player 2 wants to minimize it
        turn = 1 if state[0] == 1 else -1
        best_Qu = turn * -float('inf')
        best_act = -1

        self.Ns[state] += 1

        # Iterate over legal moves
        for i, a in enumerate(valids):
            if a:
                # Check if (s, a) in the Qsa dict
                if (state, i) in self.Qsa:
                    Qu = self.Qsa[(state, i)] + \
                        turn * self.c_ucb * \
                        np.sqrt(np.log(self.Ns[state]) /
                                (1 + self.Nsa[(state, i)]))
                else:
                    # Add small constant to not take log of 0
                    Qu = turn * np.sqrt(np.log(self.Ns[state]))

                # Update new best Qu and action based on player turn
                if turn == 1 and Qu > best_Qu:
                    best_Qu = Qu
                    best_act = i

                elif turn == -1 and Qu < best_Qu:
                    best_Qu = Qu
                    best_act = i
        s, a = state, best_act
        next_state = self.game.generate_next_state(
            s, self.game.one_hot_to_action(a))
        reward = self.search(next_state)

        if (s, a) in self.Qsa:
            self.Qsa[(s, a)] = (self.Qsa[(s, a)] *
                                self.Nsa[(s, a)] + reward) / (self.Nsa[(s, a)] + 1)
            self.Nsa[(s, a)] += 1
        else:
            self.Qsa[(s, a)] = reward
            self.Nsa[(s, a)] = 1

        return reward

    def rollout(self, state):

        while True:
            action = self.get_ann_action(state)
            state = self.game.generate_next_state(state, action)

            # Check if the state is a terminal state
            game_result = self.game.check_game_ended(state)
            if game_result:
                # Return +1 if player 1 won the game,
                # and return -1 if player 2 won the game
                return 1 if game_result == 1 else -1

    def get_ann_action(self, state):

        if state not in self.Ps:
            nn_input = self.ann.convert_state_to_input(state, self.size)
            with torch.no_grad():
                preds = self.ann.forward(nn_input).detach().numpy().flatten()
            self.Ps[state] = preds
        else:
            preds = self.Ps[state]

        valids = self.game.generate_legal_moves(state)
        valid_preds = preds * valids  # Mask illegal moves
        sum_Ps = np.sum(valid_preds)
        if sum_Ps > 0:
            valid_preds /= sum_Ps  # renormalize
        else:
            # if all valid moves were masked make all valid moves equally probable
            print("All valid moves were masked, nn may be overfitting.")
            valid_preds += valids
            valid_preds /= np.sum(valid_preds)

        r = np.random.rand()
        if r < self.eps:
            #sum_p = np.sum(valids)
            # Choose a random valid action
            action = np.random.choice(
                len(valid_preds), p=valid_preds)
            return self.game.one_hot_to_action(action)
        else:
            action = np.argmax(valid_preds)
            return self.game.one_hot_to_action(action)
